{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8595013b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guiku\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,385</span> (17.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,385\u001b[0m (17.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,385</span> (17.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,385\u001b[0m (17.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "14/14 - 1s - 66ms/step - loss: 257.7102 - val_loss: 439.2746\n",
      "Epoch 2/500\n",
      "14/14 - 0s - 6ms/step - loss: 256.9594 - val_loss: 439.0483\n",
      "Epoch 3/500\n",
      "14/14 - 0s - 5ms/step - loss: 256.5749 - val_loss: 438.6659\n",
      "Epoch 4/500\n",
      "14/14 - 0s - 5ms/step - loss: 256.3350 - val_loss: 438.4789\n",
      "Epoch 5/500\n",
      "14/14 - 0s - 5ms/step - loss: 256.1180 - val_loss: 438.3264\n",
      "Epoch 6/500\n",
      "14/14 - 0s - 5ms/step - loss: 256.0154 - val_loss: 438.2336\n",
      "Epoch 7/500\n",
      "14/14 - 0s - 5ms/step - loss: 255.9205 - val_loss: 438.1427\n",
      "Epoch 8/500\n",
      "14/14 - 0s - 5ms/step - loss: 255.7217 - val_loss: 437.7246\n",
      "Epoch 9/500\n",
      "14/14 - 0s - 5ms/step - loss: 255.3889 - val_loss: 437.5111\n",
      "Epoch 10/500\n",
      "14/14 - 0s - 5ms/step - loss: 255.1165 - val_loss: 437.2270\n",
      "Epoch 11/500\n",
      "14/14 - 0s - 5ms/step - loss: 254.7805 - val_loss: 436.9188\n",
      "Epoch 12/500\n",
      "14/14 - 0s - 5ms/step - loss: 254.5329 - val_loss: 436.6957\n",
      "Epoch 13/500\n",
      "14/14 - 0s - 5ms/step - loss: 254.3416 - val_loss: 436.5145\n",
      "Epoch 14/500\n",
      "14/14 - 0s - 6ms/step - loss: 254.1224 - val_loss: 436.2688\n",
      "Epoch 15/500\n",
      "14/14 - 0s - 5ms/step - loss: 253.9172 - val_loss: 436.0941\n",
      "Epoch 16/500\n",
      "14/14 - 0s - 5ms/step - loss: 253.6685 - val_loss: 435.7470\n",
      "Epoch 17/500\n",
      "14/14 - 0s - 5ms/step - loss: 253.3817 - val_loss: 435.5396\n",
      "Epoch 18/500\n",
      "14/14 - 0s - 5ms/step - loss: 253.1770 - val_loss: 435.3408\n",
      "Epoch 19/500\n",
      "14/14 - 0s - 5ms/step - loss: 252.9816 - val_loss: 435.1498\n",
      "Epoch 20/500\n",
      "14/14 - 0s - 5ms/step - loss: 252.7931 - val_loss: 434.9647\n",
      "Epoch 21/500\n",
      "14/14 - 0s - 5ms/step - loss: 252.6099 - val_loss: 434.7839\n",
      "Epoch 22/500\n",
      "14/14 - 0s - 5ms/step - loss: 252.4174 - val_loss: 434.4996\n",
      "Epoch 23/500\n",
      "14/14 - 0s - 5ms/step - loss: 252.1190 - val_loss: 434.2657\n",
      "Epoch 24/500\n",
      "14/14 - 0s - 5ms/step - loss: 251.8939 - val_loss: 434.0477\n",
      "Epoch 25/500\n",
      "14/14 - 0s - 5ms/step - loss: 251.6717 - val_loss: 433.8370\n",
      "Epoch 26/500\n",
      "14/14 - 0s - 5ms/step - loss: 251.2004 - val_loss: 433.2871\n",
      "Epoch 27/500\n",
      "14/14 - 0s - 5ms/step - loss: 250.9014 - val_loss: 433.0385\n",
      "Epoch 28/500\n",
      "14/14 - 0s - 5ms/step - loss: 250.6575 - val_loss: 432.8005\n",
      "Epoch 29/500\n",
      "14/14 - 0s - 5ms/step - loss: 250.4234 - val_loss: 432.5711\n",
      "Epoch 30/500\n",
      "14/14 - 0s - 5ms/step - loss: 250.1967 - val_loss: 432.3480\n",
      "Epoch 31/500\n",
      "14/14 - 0s - 6ms/step - loss: 249.9758 - val_loss: 432.1297\n",
      "Epoch 32/500\n",
      "14/14 - 0s - 5ms/step - loss: 249.7592 - val_loss: 431.9153\n",
      "Epoch 33/500\n",
      "14/14 - 0s - 5ms/step - loss: 249.5461 - val_loss: 431.7039\n",
      "Epoch 34/500\n",
      "14/14 - 0s - 5ms/step - loss: 249.3360 - val_loss: 431.4953\n",
      "Epoch 35/500\n",
      "14/14 - 0s - 5ms/step - loss: 249.1283 - val_loss: 431.2889\n",
      "Epoch 36/500\n",
      "14/14 - 0s - 5ms/step - loss: 248.9227 - val_loss: 431.0845\n",
      "Epoch 37/500\n",
      "14/14 - 0s - 5ms/step - loss: 248.7190 - val_loss: 430.8818\n",
      "Epoch 38/500\n",
      "14/14 - 0s - 5ms/step - loss: 248.5170 - val_loss: 430.6807\n",
      "Epoch 39/500\n",
      "14/14 - 0s - 5ms/step - loss: 248.3165 - val_loss: 430.4809\n",
      "Epoch 40/500\n",
      "14/14 - 0s - 5ms/step - loss: 248.1172 - val_loss: 430.2824\n",
      "Epoch 41/500\n",
      "14/14 - 0s - 5ms/step - loss: 247.9192 - val_loss: 430.0851\n",
      "Epoch 42/500\n",
      "14/14 - 0s - 5ms/step - loss: 247.7223 - val_loss: 429.8888\n",
      "Epoch 43/500\n",
      "14/14 - 0s - 5ms/step - loss: 247.5264 - val_loss: 429.6934\n",
      "Epoch 44/500\n",
      "14/14 - 0s - 5ms/step - loss: 247.3314 - val_loss: 429.4989\n",
      "Epoch 45/500\n",
      "14/14 - 0s - 5ms/step - loss: 247.1372 - val_loss: 429.3052\n",
      "Epoch 46/500\n",
      "14/14 - 0s - 5ms/step - loss: 246.9439 - val_loss: 429.1124\n",
      "Epoch 47/500\n",
      "14/14 - 0s - 5ms/step - loss: 246.7513 - val_loss: 428.9201\n",
      "Epoch 48/500\n",
      "14/14 - 0s - 5ms/step - loss: 246.5593 - val_loss: 428.7286\n",
      "Epoch 49/500\n",
      "14/14 - 0s - 5ms/step - loss: 246.3680 - val_loss: 428.5376\n",
      "Epoch 50/500\n",
      "14/14 - 0s - 5ms/step - loss: 246.1772 - val_loss: 428.3472\n",
      "Epoch 51/500\n",
      "14/14 - 0s - 5ms/step - loss: 245.9871 - val_loss: 428.1574\n",
      "Epoch 52/500\n",
      "14/14 - 0s - 5ms/step - loss: 245.7975 - val_loss: 427.9681\n",
      "Epoch 53/500\n",
      "14/14 - 0s - 5ms/step - loss: 245.6084 - val_loss: 427.7792\n",
      "Epoch 54/500\n",
      "14/14 - 0s - 5ms/step - loss: 245.4197 - val_loss: 427.5908\n",
      "Epoch 55/500\n",
      "14/14 - 0s - 6ms/step - loss: 245.2315 - val_loss: 427.4028\n",
      "Epoch 56/500\n",
      "14/14 - 0s - 6ms/step - loss: 245.0437 - val_loss: 427.2152\n",
      "Epoch 57/500\n",
      "14/14 - 0s - 5ms/step - loss: 244.8563 - val_loss: 427.0281\n",
      "Epoch 58/500\n",
      "14/14 - 0s - 6ms/step - loss: 244.6540 - val_loss: 426.7209\n",
      "Epoch 59/500\n",
      "14/14 - 0s - 5ms/step - loss: 244.1746 - val_loss: 426.3401\n",
      "Epoch 60/500\n",
      "14/14 - 0s - 5ms/step - loss: 243.9784 - val_loss: 426.1467\n",
      "Epoch 61/500\n",
      "14/14 - 0s - 5ms/step - loss: 243.7850 - val_loss: 425.9534\n",
      "Epoch 62/500\n",
      "14/14 - 0s - 5ms/step - loss: 243.5918 - val_loss: 425.7605\n",
      "Epoch 63/500\n",
      "14/14 - 0s - 5ms/step - loss: 243.3991 - val_loss: 425.5680\n",
      "Epoch 64/500\n",
      "14/14 - 0s - 5ms/step - loss: 243.2068 - val_loss: 425.3759\n",
      "Epoch 65/500\n",
      "14/14 - 0s - 5ms/step - loss: 243.0148 - val_loss: 425.1842\n",
      "Epoch 66/500\n",
      "14/14 - 0s - 5ms/step - loss: 242.8233 - val_loss: 424.9929\n",
      "Epoch 67/500\n",
      "14/14 - 0s - 6ms/step - loss: 242.6322 - val_loss: 424.8019\n",
      "Epoch 68/500\n",
      "14/14 - 0s - 5ms/step - loss: 242.4413 - val_loss: 424.6114\n",
      "Epoch 69/500\n",
      "14/14 - 0s - 5ms/step - loss: 242.2509 - val_loss: 424.4211\n",
      "Epoch 70/500\n",
      "14/14 - 0s - 5ms/step - loss: 242.0608 - val_loss: 424.2312\n",
      "Epoch 71/500\n",
      "14/14 - 0s - 5ms/step - loss: 241.8710 - val_loss: 424.0416\n",
      "Epoch 72/500\n",
      "14/14 - 0s - 5ms/step - loss: 241.6815 - val_loss: 423.8523\n",
      "Epoch 73/500\n",
      "14/14 - 0s - 5ms/step - loss: 241.4923 - val_loss: 423.6632\n",
      "Epoch 74/500\n",
      "14/14 - 0s - 5ms/step - loss: 241.3034 - val_loss: 423.4745\n",
      "Epoch 75/500\n",
      "14/14 - 0s - 6ms/step - loss: 241.1147 - val_loss: 423.2859\n",
      "Epoch 76/500\n",
      "14/14 - 0s - 5ms/step - loss: 240.9263 - val_loss: 423.0977\n",
      "Epoch 77/500\n",
      "14/14 - 0s - 6ms/step - loss: 240.7382 - val_loss: 422.9096\n",
      "Epoch 78/500\n",
      "14/14 - 0s - 6ms/step - loss: 240.5502 - val_loss: 422.7218\n",
      "Epoch 79/500\n",
      "14/14 - 0s - 6ms/step - loss: 240.3626 - val_loss: 422.5343\n",
      "Epoch 80/500\n",
      "14/14 - 0s - 6ms/step - loss: 240.1751 - val_loss: 422.3470\n",
      "Epoch 81/500\n",
      "14/14 - 0s - 6ms/step - loss: 239.9879 - val_loss: 422.1598\n",
      "Epoch 82/500\n",
      "14/14 - 0s - 5ms/step - loss: 239.8008 - val_loss: 421.9729\n",
      "Epoch 83/500\n",
      "14/14 - 0s - 5ms/step - loss: 239.6140 - val_loss: 421.7862\n",
      "Epoch 84/500\n",
      "14/14 - 0s - 5ms/step - loss: 239.4272 - val_loss: 421.5997\n",
      "Epoch 85/500\n",
      "14/14 - 0s - 5ms/step - loss: 239.2408 - val_loss: 421.4132\n",
      "Epoch 86/500\n",
      "14/14 - 0s - 5ms/step - loss: 239.0545 - val_loss: 421.2271\n",
      "Epoch 87/500\n",
      "14/14 - 0s - 5ms/step - loss: 238.8684 - val_loss: 421.0411\n",
      "Epoch 88/500\n",
      "14/14 - 0s - 5ms/step - loss: 238.6825 - val_loss: 420.8552\n",
      "Epoch 89/500\n",
      "14/14 - 0s - 5ms/step - loss: 238.4966 - val_loss: 420.6695\n",
      "Epoch 90/500\n",
      "14/14 - 0s - 5ms/step - loss: 238.3110 - val_loss: 420.4840\n",
      "Epoch 91/500\n",
      "14/14 - 0s - 5ms/step - loss: 238.1256 - val_loss: 420.2986\n",
      "Epoch 92/500\n",
      "14/14 - 0s - 5ms/step - loss: 237.9402 - val_loss: 420.1134\n",
      "Epoch 93/500\n",
      "14/14 - 0s - 5ms/step - loss: 237.7551 - val_loss: 419.9283\n",
      "Epoch 94/500\n",
      "14/14 - 0s - 5ms/step - loss: 237.5700 - val_loss: 419.7433\n",
      "Epoch 95/500\n",
      "14/14 - 0s - 5ms/step - loss: 237.3851 - val_loss: 419.5584\n",
      "Epoch 96/500\n",
      "14/14 - 0s - 5ms/step - loss: 237.2003 - val_loss: 419.3738\n",
      "Epoch 97/500\n",
      "14/14 - 0s - 5ms/step - loss: 237.0157 - val_loss: 419.1892\n",
      "Epoch 98/500\n",
      "14/14 - 0s - 5ms/step - loss: 236.8312 - val_loss: 419.0048\n",
      "Epoch 99/500\n",
      "14/14 - 0s - 5ms/step - loss: 236.6468 - val_loss: 418.8205\n",
      "Epoch 100/500\n",
      "14/14 - 0s - 5ms/step - loss: 236.4625 - val_loss: 418.6362\n",
      "Epoch 101/500\n",
      "14/14 - 0s - 5ms/step - loss: 236.2784 - val_loss: 418.4521\n",
      "Epoch 102/500\n",
      "14/14 - 0s - 5ms/step - loss: 236.0943 - val_loss: 418.2682\n",
      "Epoch 103/500\n",
      "14/14 - 0s - 5ms/step - loss: 235.9104 - val_loss: 418.0843\n",
      "Epoch 104/500\n",
      "14/14 - 0s - 5ms/step - loss: 235.7265 - val_loss: 417.9005\n",
      "Epoch 105/500\n",
      "14/14 - 0s - 5ms/step - loss: 235.5427 - val_loss: 417.7169\n",
      "Epoch 106/500\n",
      "14/14 - 0s - 5ms/step - loss: 235.3591 - val_loss: 417.5333\n",
      "Epoch 107/500\n",
      "14/14 - 0s - 6ms/step - loss: 235.1756 - val_loss: 417.3498\n",
      "Epoch 108/500\n",
      "14/14 - 0s - 5ms/step - loss: 234.9922 - val_loss: 417.1664\n",
      "Epoch 109/500\n",
      "14/14 - 0s - 5ms/step - loss: 234.8088 - val_loss: 416.9831\n",
      "Epoch 110/500\n",
      "14/14 - 0s - 5ms/step - loss: 234.6255 - val_loss: 416.7999\n",
      "Epoch 111/500\n",
      "14/14 - 0s - 5ms/step - loss: 234.4424 - val_loss: 416.6168\n",
      "Epoch 112/500\n",
      "14/14 - 0s - 5ms/step - loss: 234.2593 - val_loss: 416.4337\n",
      "Epoch 113/500\n",
      "14/14 - 0s - 5ms/step - loss: 234.0763 - val_loss: 416.2508\n",
      "Epoch 114/500\n",
      "14/14 - 0s - 5ms/step - loss: 233.8934 - val_loss: 416.0679\n",
      "Epoch 115/500\n",
      "14/14 - 0s - 5ms/step - loss: 233.7105 - val_loss: 415.8851\n",
      "Epoch 116/500\n",
      "14/14 - 0s - 5ms/step - loss: 233.5278 - val_loss: 415.7024\n",
      "Epoch 117/500\n",
      "14/14 - 0s - 5ms/step - loss: 233.3451 - val_loss: 415.5197\n",
      "Epoch 118/500\n",
      "14/14 - 0s - 5ms/step - loss: 233.1625 - val_loss: 415.3372\n",
      "Epoch 119/500\n",
      "14/14 - 0s - 5ms/step - loss: 232.9799 - val_loss: 415.1547\n",
      "Epoch 120/500\n",
      "14/14 - 0s - 5ms/step - loss: 232.7975 - val_loss: 414.9723\n",
      "Epoch 121/500\n",
      "14/14 - 0s - 5ms/step - loss: 232.6118 - val_loss: 414.7436\n",
      "Epoch 122/500\n",
      "14/14 - 0s - 5ms/step - loss: 232.2039 - val_loss: 414.3141\n",
      "Epoch 123/500\n",
      "14/14 - 0s - 5ms/step - loss: 231.9213 - val_loss: 414.0520\n",
      "Epoch 124/500\n",
      "14/14 - 0s - 5ms/step - loss: 231.6645 - val_loss: 413.8019\n",
      "Epoch 125/500\n",
      "14/14 - 0s - 5ms/step - loss: 231.4188 - val_loss: 413.5613\n",
      "Epoch 126/500\n",
      "14/14 - 0s - 5ms/step - loss: 231.1812 - val_loss: 413.3272\n",
      "Epoch 127/500\n",
      "14/14 - 0s - 5ms/step - loss: 230.9494 - val_loss: 413.0981\n",
      "Epoch 128/500\n",
      "14/14 - 0s - 6ms/step - loss: 230.7218 - val_loss: 412.8725\n",
      "Epoch 129/500\n",
      "14/14 - 0s - 5ms/step - loss: 230.4977 - val_loss: 412.6500\n",
      "Epoch 130/500\n",
      "14/14 - 0s - 5ms/step - loss: 230.2761 - val_loss: 412.4297\n",
      "Epoch 131/500\n",
      "14/14 - 0s - 5ms/step - loss: 230.0568 - val_loss: 412.2115\n",
      "Epoch 132/500\n",
      "14/14 - 0s - 5ms/step - loss: 229.8393 - val_loss: 411.9949\n",
      "Epoch 133/500\n",
      "14/14 - 0s - 5ms/step - loss: 229.6234 - val_loss: 411.7798\n",
      "Epoch 134/500\n",
      "14/14 - 0s - 5ms/step - loss: 229.4088 - val_loss: 411.5660\n",
      "Epoch 135/500\n",
      "14/14 - 0s - 5ms/step - loss: 229.1956 - val_loss: 411.3533\n",
      "Epoch 136/500\n",
      "14/14 - 0s - 5ms/step - loss: 228.9833 - val_loss: 411.1417\n",
      "Epoch 137/500\n",
      "14/14 - 0s - 5ms/step - loss: 228.7720 - val_loss: 410.9309\n",
      "Epoch 138/500\n",
      "14/14 - 0s - 5ms/step - loss: 228.5616 - val_loss: 410.7209\n",
      "Epoch 139/500\n",
      "14/14 - 0s - 5ms/step - loss: 228.3520 - val_loss: 410.5118\n",
      "Epoch 140/500\n",
      "14/14 - 0s - 5ms/step - loss: 228.1431 - val_loss: 410.3033\n",
      "Epoch 141/500\n",
      "14/14 - 0s - 5ms/step - loss: 227.9349 - val_loss: 410.0953\n",
      "Epoch 142/500\n",
      "14/14 - 0s - 5ms/step - loss: 227.7273 - val_loss: 409.8881\n",
      "Epoch 143/500\n",
      "14/14 - 0s - 5ms/step - loss: 227.5202 - val_loss: 409.6814\n",
      "Epoch 144/500\n",
      "14/14 - 0s - 5ms/step - loss: 227.3137 - val_loss: 409.4752\n",
      "Epoch 145/500\n",
      "14/14 - 0s - 5ms/step - loss: 227.1077 - val_loss: 409.2694\n",
      "Epoch 146/500\n",
      "14/14 - 0s - 5ms/step - loss: 226.9022 - val_loss: 409.0641\n",
      "Epoch 147/500\n",
      "14/14 - 0s - 5ms/step - loss: 226.6971 - val_loss: 408.8592\n",
      "Epoch 148/500\n",
      "14/14 - 0s - 5ms/step - loss: 226.4923 - val_loss: 408.6547\n",
      "Epoch 149/500\n",
      "14/14 - 0s - 5ms/step - loss: 226.2880 - val_loss: 408.4506\n",
      "Epoch 150/500\n",
      "14/14 - 0s - 5ms/step - loss: 226.0841 - val_loss: 408.2469\n",
      "Epoch 151/500\n",
      "14/14 - 0s - 5ms/step - loss: 225.8804 - val_loss: 408.0434\n",
      "Epoch 152/500\n",
      "14/14 - 0s - 5ms/step - loss: 225.6771 - val_loss: 407.8403\n",
      "Epoch 153/500\n",
      "14/14 - 0s - 5ms/step - loss: 225.4742 - val_loss: 407.6375\n",
      "Epoch 154/500\n",
      "14/14 - 0s - 5ms/step - loss: 225.2715 - val_loss: 407.4350\n",
      "Epoch 155/500\n",
      "14/14 - 0s - 4ms/step - loss: 225.0690 - val_loss: 407.2327\n",
      "Epoch 156/500\n",
      "14/14 - 0s - 5ms/step - loss: 224.8669 - val_loss: 407.0307\n",
      "Epoch 157/500\n",
      "14/14 - 0s - 5ms/step - loss: 224.6650 - val_loss: 406.8289\n",
      "Epoch 158/500\n",
      "14/14 - 0s - 5ms/step - loss: 224.4634 - val_loss: 406.6274\n",
      "Epoch 159/500\n",
      "14/14 - 0s - 5ms/step - loss: 224.2620 - val_loss: 406.4262\n",
      "Epoch 160/500\n",
      "14/14 - 0s - 5ms/step - loss: 224.0608 - val_loss: 406.2251\n",
      "Epoch 161/500\n",
      "14/14 - 0s - 5ms/step - loss: 223.8598 - val_loss: 406.0243\n",
      "Epoch 162/500\n",
      "14/14 - 0s - 5ms/step - loss: 223.6591 - val_loss: 405.8236\n",
      "Epoch 163/500\n",
      "14/14 - 0s - 5ms/step - loss: 223.4585 - val_loss: 405.6232\n",
      "Epoch 164/500\n",
      "14/14 - 0s - 5ms/step - loss: 223.2581 - val_loss: 405.4229\n",
      "Epoch 165/500\n",
      "14/14 - 0s - 5ms/step - loss: 223.0580 - val_loss: 405.2228\n",
      "Epoch 166/500\n",
      "14/14 - 0s - 5ms/step - loss: 222.8579 - val_loss: 405.0229\n",
      "Epoch 167/500\n",
      "14/14 - 0s - 5ms/step - loss: 222.6581 - val_loss: 404.8231\n",
      "Epoch 168/500\n",
      "14/14 - 0s - 6ms/step - loss: 222.4584 - val_loss: 404.6236\n",
      "Epoch 169/500\n",
      "14/14 - 0s - 5ms/step - loss: 222.2589 - val_loss: 404.4242\n",
      "Epoch 170/500\n",
      "14/14 - 0s - 6ms/step - loss: 222.0596 - val_loss: 404.2249\n",
      "Epoch 171/500\n",
      "14/14 - 0s - 6ms/step - loss: 221.8604 - val_loss: 404.0258\n",
      "Epoch 172/500\n",
      "14/14 - 0s - 6ms/step - loss: 221.6613 - val_loss: 403.8268\n",
      "Epoch 173/500\n",
      "14/14 - 0s - 6ms/step - loss: 221.4623 - val_loss: 403.6280\n",
      "Epoch 174/500\n",
      "14/14 - 0s - 5ms/step - loss: 221.0275 - val_loss: 402.7782\n",
      "Epoch 175/500\n",
      "14/14 - 0s - 5ms/step - loss: 220.4113 - val_loss: 402.5732\n",
      "Epoch 176/500\n",
      "14/14 - 0s - 5ms/step - loss: 220.2058 - val_loss: 402.3676\n",
      "Epoch 177/500\n",
      "14/14 - 0s - 5ms/step - loss: 220.0001 - val_loss: 402.1618\n",
      "Epoch 178/500\n",
      "14/14 - 0s - 6ms/step - loss: 219.7945 - val_loss: 401.9563\n",
      "Epoch 179/500\n",
      "14/14 - 0s - 7ms/step - loss: 219.5890 - val_loss: 401.7509\n",
      "Epoch 180/500\n",
      "14/14 - 0s - 5ms/step - loss: 219.3837 - val_loss: 401.5457\n",
      "Epoch 181/500\n",
      "14/14 - 0s - 5ms/step - loss: 219.1787 - val_loss: 401.3408\n",
      "Epoch 182/500\n",
      "14/14 - 0s - 5ms/step - loss: 218.9738 - val_loss: 401.1360\n",
      "Epoch 183/500\n",
      "14/14 - 0s - 5ms/step - loss: 218.7691 - val_loss: 400.9314\n",
      "Epoch 184/500\n",
      "14/14 - 0s - 5ms/step - loss: 218.5645 - val_loss: 400.7270\n",
      "Epoch 185/500\n",
      "14/14 - 0s - 5ms/step - loss: 218.3602 - val_loss: 400.5227\n",
      "Epoch 186/500\n",
      "14/14 - 0s - 5ms/step - loss: 218.1560 - val_loss: 400.3186\n",
      "Epoch 187/500\n",
      "14/14 - 0s - 5ms/step - loss: 217.9520 - val_loss: 400.1147\n",
      "Epoch 188/500\n",
      "14/14 - 0s - 5ms/step - loss: 217.7481 - val_loss: 399.9109\n",
      "Epoch 189/500\n",
      "14/14 - 0s - 5ms/step - loss: 217.5444 - val_loss: 399.7073\n",
      "Epoch 190/500\n",
      "14/14 - 0s - 7ms/step - loss: 217.3409 - val_loss: 399.5038\n",
      "Epoch 191/500\n",
      "14/14 - 0s - 6ms/step - loss: 217.1375 - val_loss: 399.3005\n",
      "Epoch 192/500\n",
      "14/14 - 0s - 5ms/step - loss: 216.9342 - val_loss: 399.0973\n",
      "Epoch 193/500\n",
      "14/14 - 0s - 4ms/step - loss: 216.7311 - val_loss: 398.8943\n",
      "Epoch 194/500\n",
      "14/14 - 0s - 5ms/step - loss: 216.5281 - val_loss: 398.6914\n",
      "Epoch 195/500\n",
      "14/14 - 0s - 5ms/step - loss: 216.3252 - val_loss: 398.4886\n",
      "Epoch 196/500\n",
      "14/14 - 0s - 5ms/step - loss: 216.1225 - val_loss: 398.2860\n",
      "Epoch 197/500\n",
      "14/14 - 0s - 5ms/step - loss: 215.9199 - val_loss: 398.0834\n",
      "Epoch 198/500\n",
      "14/14 - 0s - 5ms/step - loss: 215.7175 - val_loss: 397.8810\n",
      "Epoch 199/500\n",
      "14/14 - 0s - 5ms/step - loss: 215.5151 - val_loss: 397.6788\n",
      "Epoch 200/500\n",
      "14/14 - 0s - 5ms/step - loss: 215.3129 - val_loss: 397.4766\n",
      "Epoch 201/500\n",
      "14/14 - 0s - 5ms/step - loss: 215.1108 - val_loss: 397.2745\n",
      "Epoch 202/500\n",
      "14/14 - 0s - 5ms/step - loss: 214.9088 - val_loss: 397.0726\n",
      "Epoch 203/500\n",
      "14/14 - 0s - 5ms/step - loss: 214.7069 - val_loss: 396.8707\n",
      "Epoch 204/500\n",
      "14/14 - 0s - 5ms/step - loss: 214.5051 - val_loss: 396.6691\n",
      "Epoch 205/500\n",
      "14/14 - 0s - 5ms/step - loss: 214.3034 - val_loss: 396.4674\n",
      "Epoch 206/500\n",
      "14/14 - 0s - 5ms/step - loss: 214.1018 - val_loss: 396.2659\n",
      "Epoch 207/500\n",
      "14/14 - 0s - 5ms/step - loss: 213.9004 - val_loss: 396.0645\n",
      "Epoch 208/500\n",
      "14/14 - 0s - 5ms/step - loss: 213.6990 - val_loss: 395.8631\n",
      "Epoch 209/500\n",
      "14/14 - 0s - 5ms/step - loss: 213.4977 - val_loss: 395.6619\n",
      "Epoch 210/500\n",
      "14/14 - 0s - 5ms/step - loss: 213.2965 - val_loss: 395.4608\n",
      "Epoch 211/500\n",
      "14/14 - 0s - 5ms/step - loss: 213.0954 - val_loss: 395.2598\n",
      "Epoch 212/500\n",
      "14/14 - 0s - 5ms/step - loss: 212.8944 - val_loss: 395.0587\n",
      "Epoch 213/500\n",
      "14/14 - 0s - 5ms/step - loss: 212.6935 - val_loss: 394.8579\n",
      "Epoch 214/500\n",
      "14/14 - 0s - 5ms/step - loss: 212.4926 - val_loss: 394.6571\n",
      "Epoch 215/500\n",
      "14/14 - 0s - 5ms/step - loss: 212.2919 - val_loss: 394.4564\n",
      "Epoch 216/500\n",
      "14/14 - 0s - 6ms/step - loss: 212.0912 - val_loss: 394.2557\n",
      "Epoch 217/500\n",
      "14/14 - 0s - 5ms/step - loss: 211.8906 - val_loss: 394.0552\n",
      "Epoch 218/500\n",
      "14/14 - 0s - 5ms/step - loss: 211.6901 - val_loss: 393.8548\n",
      "Epoch 219/500\n",
      "14/14 - 0s - 5ms/step - loss: 211.4897 - val_loss: 393.6544\n",
      "Epoch 220/500\n",
      "14/14 - 0s - 5ms/step - loss: 211.2893 - val_loss: 393.4541\n",
      "Epoch 221/500\n",
      "14/14 - 0s - 5ms/step - loss: 211.0890 - val_loss: 393.2538\n",
      "Epoch 222/500\n",
      "14/14 - 0s - 5ms/step - loss: 210.8888 - val_loss: 393.0536\n",
      "Epoch 223/500\n",
      "14/14 - 0s - 5ms/step - loss: 210.6887 - val_loss: 392.8536\n",
      "Epoch 224/500\n",
      "14/14 - 0s - 5ms/step - loss: 210.4886 - val_loss: 392.6535\n",
      "Epoch 225/500\n",
      "14/14 - 0s - 5ms/step - loss: 210.2886 - val_loss: 392.4535\n",
      "Epoch 226/500\n",
      "14/14 - 0s - 5ms/step - loss: 210.0887 - val_loss: 392.2537\n",
      "Epoch 227/500\n",
      "14/14 - 0s - 5ms/step - loss: 209.8888 - val_loss: 392.0538\n",
      "Epoch 228/500\n",
      "14/14 - 0s - 5ms/step - loss: 209.6890 - val_loss: 391.8541\n",
      "Epoch 229/500\n",
      "14/14 - 0s - 5ms/step - loss: 209.4892 - val_loss: 391.6544\n",
      "Epoch 230/500\n",
      "14/14 - 0s - 5ms/step - loss: 209.2896 - val_loss: 391.4547\n",
      "Epoch 231/500\n",
      "14/14 - 0s - 5ms/step - loss: 209.0900 - val_loss: 391.2551\n",
      "Epoch 232/500\n",
      "14/14 - 0s - 5ms/step - loss: 208.8904 - val_loss: 391.0556\n",
      "Epoch 233/500\n",
      "14/14 - 0s - 5ms/step - loss: 208.6909 - val_loss: 390.8561\n",
      "Epoch 234/500\n",
      "14/14 - 0s - 5ms/step - loss: 208.4914 - val_loss: 390.6567\n",
      "Epoch 235/500\n",
      "14/14 - 0s - 5ms/step - loss: 208.2921 - val_loss: 390.4574\n",
      "Epoch 236/500\n",
      "14/14 - 0s - 5ms/step - loss: 208.0927 - val_loss: 390.2580\n",
      "Epoch 237/500\n",
      "14/14 - 0s - 5ms/step - loss: 207.8935 - val_loss: 390.0588\n",
      "Epoch 238/500\n",
      "14/14 - 0s - 5ms/step - loss: 207.6942 - val_loss: 389.8596\n",
      "Epoch 239/500\n",
      "14/14 - 0s - 5ms/step - loss: 207.4951 - val_loss: 389.6604\n",
      "Epoch 240/500\n",
      "14/14 - 0s - 5ms/step - loss: 207.2959 - val_loss: 389.4614\n",
      "Epoch 241/500\n",
      "14/14 - 0s - 5ms/step - loss: 207.0969 - val_loss: 389.2624\n",
      "Epoch 242/500\n",
      "14/14 - 0s - 5ms/step - loss: 206.8978 - val_loss: 389.0633\n",
      "Epoch 243/500\n",
      "14/14 - 0s - 5ms/step - loss: 206.6989 - val_loss: 388.8644\n",
      "Epoch 244/500\n",
      "14/14 - 0s - 5ms/step - loss: 206.4999 - val_loss: 388.6655\n",
      "Epoch 245/500\n",
      "14/14 - 0s - 5ms/step - loss: 206.3011 - val_loss: 388.4666\n",
      "Epoch 246/500\n",
      "14/14 - 0s - 5ms/step - loss: 206.1022 - val_loss: 388.2678\n",
      "Epoch 247/500\n",
      "14/14 - 0s - 5ms/step - loss: 205.9035 - val_loss: 388.0691\n",
      "Epoch 248/500\n",
      "14/14 - 0s - 5ms/step - loss: 205.7047 - val_loss: 387.8703\n",
      "Epoch 249/500\n",
      "14/14 - 0s - 5ms/step - loss: 205.5060 - val_loss: 387.6717\n",
      "Epoch 250/500\n",
      "14/14 - 0s - 5ms/step - loss: 205.3073 - val_loss: 387.4731\n",
      "Epoch 251/500\n",
      "14/14 - 0s - 5ms/step - loss: 205.1087 - val_loss: 387.2745\n",
      "Epoch 252/500\n",
      "14/14 - 0s - 5ms/step - loss: 204.9102 - val_loss: 387.0759\n",
      "Epoch 253/500\n",
      "14/14 - 0s - 5ms/step - loss: 204.7116 - val_loss: 386.8774\n",
      "Epoch 254/500\n",
      "14/14 - 0s - 5ms/step - loss: 204.5131 - val_loss: 386.6789\n",
      "Epoch 255/500\n",
      "14/14 - 0s - 5ms/step - loss: 204.3146 - val_loss: 386.4805\n",
      "Epoch 256/500\n",
      "14/14 - 0s - 5ms/step - loss: 204.1163 - val_loss: 386.2821\n",
      "Epoch 257/500\n",
      "14/14 - 0s - 5ms/step - loss: 203.9178 - val_loss: 386.0837\n",
      "Epoch 258/500\n",
      "14/14 - 0s - 5ms/step - loss: 203.7195 - val_loss: 385.8854\n",
      "Epoch 259/500\n",
      "14/14 - 0s - 6ms/step - loss: 203.5212 - val_loss: 385.6871\n",
      "Epoch 260/500\n",
      "14/14 - 0s - 5ms/step - loss: 203.3230 - val_loss: 385.4889\n",
      "Epoch 261/500\n",
      "14/14 - 0s - 6ms/step - loss: 203.1247 - val_loss: 385.2906\n",
      "Epoch 262/500\n",
      "14/14 - 0s - 5ms/step - loss: 202.9265 - val_loss: 385.0924\n",
      "Epoch 263/500\n",
      "14/14 - 0s - 6ms/step - loss: 202.7283 - val_loss: 384.8943\n",
      "Epoch 264/500\n",
      "14/14 - 0s - 5ms/step - loss: 202.5302 - val_loss: 384.6962\n",
      "Epoch 265/500\n",
      "14/14 - 0s - 5ms/step - loss: 202.3321 - val_loss: 384.4981\n",
      "Epoch 266/500\n",
      "14/14 - 0s - 5ms/step - loss: 202.1340 - val_loss: 384.3000\n",
      "Epoch 267/500\n",
      "14/14 - 0s - 5ms/step - loss: 201.9360 - val_loss: 384.1020\n",
      "Epoch 268/500\n",
      "14/14 - 0s - 5ms/step - loss: 201.7380 - val_loss: 383.9041\n",
      "Epoch 269/500\n",
      "14/14 - 0s - 5ms/step - loss: 201.5400 - val_loss: 383.7061\n",
      "Epoch 270/500\n",
      "14/14 - 0s - 5ms/step - loss: 201.3421 - val_loss: 383.5082\n",
      "Epoch 271/500\n",
      "14/14 - 0s - 5ms/step - loss: 201.1441 - val_loss: 383.3103\n",
      "Epoch 272/500\n",
      "14/14 - 0s - 5ms/step - loss: 200.9462 - val_loss: 383.1124\n",
      "Epoch 273/500\n",
      "14/14 - 0s - 5ms/step - loss: 200.7484 - val_loss: 382.9146\n",
      "Epoch 274/500\n",
      "14/14 - 0s - 5ms/step - loss: 200.5506 - val_loss: 382.7168\n",
      "Epoch 275/500\n",
      "14/14 - 0s - 6ms/step - loss: 200.3528 - val_loss: 382.5190\n",
      "Epoch 276/500\n",
      "14/14 - 0s - 5ms/step - loss: 200.1550 - val_loss: 382.3212\n",
      "Epoch 277/500\n",
      "14/14 - 0s - 5ms/step - loss: 199.9573 - val_loss: 382.1234\n",
      "Epoch 278/500\n",
      "14/14 - 0s - 5ms/step - loss: 199.7595 - val_loss: 381.9258\n",
      "Epoch 279/500\n",
      "14/14 - 0s - 5ms/step - loss: 199.5618 - val_loss: 381.7281\n",
      "Epoch 280/500\n",
      "14/14 - 0s - 5ms/step - loss: 199.3642 - val_loss: 381.5304\n",
      "Epoch 281/500\n",
      "14/14 - 0s - 5ms/step - loss: 199.1665 - val_loss: 381.3328\n",
      "Epoch 282/500\n",
      "14/14 - 0s - 5ms/step - loss: 198.9689 - val_loss: 381.1352\n",
      "Epoch 283/500\n",
      "14/14 - 0s - 5ms/step - loss: 198.7713 - val_loss: 380.9377\n",
      "Epoch 284/500\n",
      "14/14 - 0s - 5ms/step - loss: 198.5737 - val_loss: 380.7401\n",
      "Epoch 285/500\n",
      "14/14 - 0s - 5ms/step - loss: 198.3762 - val_loss: 380.5425\n",
      "Epoch 286/500\n",
      "14/14 - 0s - 5ms/step - loss: 198.1787 - val_loss: 380.3451\n",
      "Epoch 287/500\n",
      "14/14 - 0s - 5ms/step - loss: 197.9811 - val_loss: 380.1475\n",
      "Epoch 288/500\n",
      "14/14 - 0s - 5ms/step - loss: 197.7837 - val_loss: 379.9500\n",
      "Epoch 289/500\n",
      "14/14 - 0s - 5ms/step - loss: 197.5862 - val_loss: 379.7526\n",
      "Epoch 290/500\n",
      "14/14 - 0s - 5ms/step - loss: 197.3888 - val_loss: 379.5552\n",
      "Epoch 291/500\n",
      "14/14 - 0s - 5ms/step - loss: 197.1914 - val_loss: 379.3578\n",
      "Epoch 292/500\n",
      "14/14 - 0s - 5ms/step - loss: 196.9940 - val_loss: 379.1604\n",
      "Epoch 293/500\n",
      "14/14 - 0s - 5ms/step - loss: 196.7966 - val_loss: 378.9630\n",
      "Epoch 294/500\n",
      "14/14 - 0s - 5ms/step - loss: 196.5992 - val_loss: 378.7657\n",
      "Epoch 295/500\n",
      "14/14 - 0s - 5ms/step - loss: 196.4019 - val_loss: 378.5684\n",
      "Epoch 296/500\n",
      "14/14 - 0s - 5ms/step - loss: 196.2046 - val_loss: 378.3711\n",
      "Epoch 297/500\n",
      "14/14 - 0s - 5ms/step - loss: 196.0073 - val_loss: 378.1738\n",
      "Epoch 298/500\n",
      "14/14 - 0s - 5ms/step - loss: 195.8100 - val_loss: 377.9766\n",
      "Epoch 299/500\n",
      "14/14 - 0s - 5ms/step - loss: 195.6127 - val_loss: 377.7793\n",
      "Epoch 300/500\n",
      "14/14 - 0s - 5ms/step - loss: 195.4156 - val_loss: 377.5821\n",
      "Epoch 301/500\n",
      "14/14 - 0s - 5ms/step - loss: 195.2183 - val_loss: 377.3848\n",
      "Epoch 302/500\n",
      "14/14 - 0s - 5ms/step - loss: 195.0211 - val_loss: 377.1877\n",
      "Epoch 303/500\n",
      "14/14 - 0s - 5ms/step - loss: 194.8239 - val_loss: 376.9904\n",
      "Epoch 304/500\n",
      "14/14 - 0s - 5ms/step - loss: 194.6268 - val_loss: 376.7933\n",
      "Epoch 305/500\n",
      "14/14 - 0s - 5ms/step - loss: 194.4296 - val_loss: 376.5962\n",
      "Epoch 306/500\n",
      "14/14 - 0s - 5ms/step - loss: 194.2325 - val_loss: 376.3991\n",
      "Epoch 307/500\n",
      "14/14 - 0s - 5ms/step - loss: 194.0354 - val_loss: 376.2020\n",
      "Epoch 308/500\n",
      "14/14 - 0s - 5ms/step - loss: 193.8383 - val_loss: 376.0049\n",
      "Epoch 309/500\n",
      "14/14 - 0s - 5ms/step - loss: 193.6412 - val_loss: 375.8078\n",
      "Epoch 310/500\n",
      "14/14 - 0s - 5ms/step - loss: 193.4441 - val_loss: 375.6107\n",
      "Epoch 311/500\n",
      "14/14 - 0s - 5ms/step - loss: 193.2471 - val_loss: 375.4137\n",
      "Epoch 312/500\n",
      "14/14 - 0s - 5ms/step - loss: 193.0500 - val_loss: 375.2167\n",
      "Epoch 313/500\n",
      "14/14 - 0s - 5ms/step - loss: 192.8531 - val_loss: 375.0197\n",
      "Epoch 314/500\n",
      "14/14 - 0s - 5ms/step - loss: 192.6561 - val_loss: 374.8227\n",
      "Epoch 315/500\n",
      "14/14 - 0s - 5ms/step - loss: 192.4591 - val_loss: 374.6257\n",
      "Epoch 316/500\n",
      "14/14 - 0s - 5ms/step - loss: 192.2621 - val_loss: 374.4288\n",
      "Epoch 317/500\n",
      "14/14 - 0s - 5ms/step - loss: 192.0651 - val_loss: 374.2318\n",
      "Epoch 318/500\n",
      "14/14 - 0s - 5ms/step - loss: 191.8682 - val_loss: 374.0349\n",
      "Epoch 319/500\n",
      "14/14 - 0s - 5ms/step - loss: 191.6713 - val_loss: 373.8380\n",
      "Epoch 320/500\n",
      "14/14 - 0s - 5ms/step - loss: 191.4743 - val_loss: 373.6411\n",
      "Epoch 321/500\n",
      "14/14 - 0s - 5ms/step - loss: 191.2775 - val_loss: 373.4442\n",
      "Epoch 322/500\n",
      "14/14 - 0s - 5ms/step - loss: 191.0806 - val_loss: 373.2473\n",
      "Epoch 323/500\n",
      "14/14 - 0s - 5ms/step - loss: 190.8837 - val_loss: 373.0504\n",
      "Epoch 324/500\n",
      "14/14 - 0s - 5ms/step - loss: 190.6868 - val_loss: 372.8536\n",
      "Epoch 325/500\n",
      "14/14 - 0s - 5ms/step - loss: 190.4900 - val_loss: 372.6567\n",
      "Epoch 326/500\n",
      "14/14 - 0s - 5ms/step - loss: 190.2931 - val_loss: 372.4598\n",
      "Epoch 327/500\n",
      "14/14 - 0s - 5ms/step - loss: 190.0963 - val_loss: 372.2631\n",
      "Epoch 328/500\n",
      "14/14 - 0s - 5ms/step - loss: 189.8995 - val_loss: 372.0662\n",
      "Epoch 329/500\n",
      "14/14 - 0s - 5ms/step - loss: 189.7027 - val_loss: 371.8694\n",
      "Epoch 330/500\n",
      "14/14 - 0s - 5ms/step - loss: 189.5059 - val_loss: 371.6727\n",
      "Epoch 331/500\n",
      "14/14 - 0s - 5ms/step - loss: 189.3091 - val_loss: 371.4758\n",
      "Epoch 332/500\n",
      "14/14 - 0s - 5ms/step - loss: 189.1123 - val_loss: 371.2791\n",
      "Epoch 333/500\n",
      "14/14 - 0s - 5ms/step - loss: 188.9155 - val_loss: 371.0823\n",
      "Epoch 334/500\n",
      "14/14 - 0s - 6ms/step - loss: 188.7188 - val_loss: 370.8856\n",
      "Epoch 335/500\n",
      "14/14 - 0s - 6ms/step - loss: 188.5220 - val_loss: 370.6889\n",
      "Epoch 336/500\n",
      "14/14 - 0s - 5ms/step - loss: 188.3253 - val_loss: 370.4921\n",
      "Epoch 337/500\n",
      "14/14 - 0s - 5ms/step - loss: 188.1286 - val_loss: 370.2954\n",
      "Epoch 338/500\n",
      "14/14 - 0s - 5ms/step - loss: 187.9319 - val_loss: 370.0987\n",
      "Epoch 339/500\n",
      "14/14 - 0s - 5ms/step - loss: 187.7352 - val_loss: 369.9020\n",
      "Epoch 340/500\n",
      "14/14 - 0s - 5ms/step - loss: 187.5385 - val_loss: 369.7053\n",
      "Epoch 341/500\n",
      "14/14 - 0s - 5ms/step - loss: 187.3418 - val_loss: 369.5086\n",
      "Epoch 342/500\n",
      "14/14 - 0s - 5ms/step - loss: 187.1452 - val_loss: 369.3120\n",
      "Epoch 343/500\n",
      "14/14 - 0s - 5ms/step - loss: 186.9485 - val_loss: 369.1154\n",
      "Epoch 344/500\n",
      "14/14 - 0s - 5ms/step - loss: 186.7519 - val_loss: 368.9188\n",
      "Epoch 345/500\n",
      "14/14 - 0s - 5ms/step - loss: 186.5552 - val_loss: 368.7221\n",
      "Epoch 346/500\n",
      "14/14 - 0s - 6ms/step - loss: 186.3586 - val_loss: 368.5255\n",
      "Epoch 347/500\n",
      "14/14 - 0s - 5ms/step - loss: 186.1620 - val_loss: 368.3289\n",
      "Epoch 348/500\n",
      "14/14 - 0s - 5ms/step - loss: 185.9654 - val_loss: 368.1323\n",
      "Epoch 349/500\n",
      "14/14 - 0s - 5ms/step - loss: 185.7688 - val_loss: 367.9357\n",
      "Epoch 350/500\n",
      "14/14 - 0s - 5ms/step - loss: 185.5722 - val_loss: 367.7391\n",
      "Epoch 351/500\n",
      "14/14 - 0s - 5ms/step - loss: 185.3756 - val_loss: 367.5424\n",
      "Epoch 352/500\n",
      "14/14 - 0s - 5ms/step - loss: 185.1790 - val_loss: 367.3459\n",
      "Epoch 353/500\n",
      "14/14 - 0s - 5ms/step - loss: 184.9825 - val_loss: 367.1494\n",
      "Epoch 354/500\n",
      "14/14 - 0s - 5ms/step - loss: 184.7859 - val_loss: 366.9529\n",
      "Epoch 355/500\n",
      "14/14 - 0s - 5ms/step - loss: 184.5894 - val_loss: 366.7563\n",
      "Epoch 356/500\n",
      "14/14 - 0s - 5ms/step - loss: 184.3928 - val_loss: 366.5597\n",
      "Epoch 357/500\n",
      "14/14 - 0s - 5ms/step - loss: 184.1963 - val_loss: 366.3632\n",
      "Epoch 358/500\n",
      "14/14 - 0s - 5ms/step - loss: 183.9998 - val_loss: 366.1667\n",
      "Epoch 359/500\n",
      "14/14 - 0s - 5ms/step - loss: 183.8032 - val_loss: 365.9702\n",
      "Epoch 360/500\n",
      "14/14 - 0s - 5ms/step - loss: 183.6067 - val_loss: 365.7737\n",
      "Epoch 361/500\n",
      "14/14 - 0s - 5ms/step - loss: 183.4102 - val_loss: 365.5772\n",
      "Epoch 362/500\n",
      "14/14 - 0s - 5ms/step - loss: 183.2137 - val_loss: 365.3807\n",
      "Epoch 363/500\n",
      "14/14 - 0s - 5ms/step - loss: 183.0172 - val_loss: 365.1842\n",
      "Epoch 364/500\n",
      "14/14 - 0s - 5ms/step - loss: 182.8207 - val_loss: 364.9877\n",
      "Epoch 365/500\n",
      "14/14 - 0s - 5ms/step - loss: 182.6243 - val_loss: 364.7912\n",
      "Epoch 366/500\n",
      "14/14 - 0s - 5ms/step - loss: 182.4278 - val_loss: 364.5948\n",
      "Epoch 367/500\n",
      "14/14 - 0s - 5ms/step - loss: 182.2313 - val_loss: 364.3983\n",
      "Epoch 368/500\n",
      "14/14 - 0s - 5ms/step - loss: 182.0349 - val_loss: 364.2019\n",
      "Epoch 369/500\n",
      "14/14 - 0s - 5ms/step - loss: 181.8384 - val_loss: 364.0054\n",
      "Epoch 370/500\n",
      "14/14 - 0s - 5ms/step - loss: 181.6420 - val_loss: 363.8090\n",
      "Epoch 371/500\n",
      "14/14 - 0s - 5ms/step - loss: 181.4456 - val_loss: 363.6125\n",
      "Epoch 372/500\n",
      "14/14 - 0s - 5ms/step - loss: 181.2491 - val_loss: 363.4161\n",
      "Epoch 373/500\n",
      "14/14 - 0s - 5ms/step - loss: 181.0527 - val_loss: 363.2197\n",
      "Epoch 374/500\n",
      "14/14 - 0s - 5ms/step - loss: 180.8563 - val_loss: 363.0233\n",
      "Epoch 375/500\n",
      "14/14 - 0s - 5ms/step - loss: 180.6599 - val_loss: 362.8269\n",
      "Epoch 376/500\n",
      "14/14 - 0s - 5ms/step - loss: 180.4635 - val_loss: 362.6305\n",
      "Epoch 377/500\n",
      "14/14 - 0s - 5ms/step - loss: 180.2671 - val_loss: 362.4341\n",
      "Epoch 378/500\n",
      "14/14 - 0s - 5ms/step - loss: 180.0707 - val_loss: 362.2377\n",
      "Epoch 379/500\n",
      "14/14 - 0s - 5ms/step - loss: 179.8743 - val_loss: 362.0413\n",
      "Epoch 380/500\n",
      "14/14 - 0s - 5ms/step - loss: 179.6779 - val_loss: 361.8449\n",
      "Epoch 381/500\n",
      "14/14 - 0s - 5ms/step - loss: 179.4816 - val_loss: 361.6486\n",
      "Epoch 382/500\n",
      "14/14 - 0s - 5ms/step - loss: 179.2852 - val_loss: 361.4523\n",
      "Epoch 383/500\n",
      "14/14 - 0s - 5ms/step - loss: 179.0889 - val_loss: 361.2559\n",
      "Epoch 384/500\n",
      "14/14 - 0s - 5ms/step - loss: 178.8925 - val_loss: 361.0595\n",
      "Epoch 385/500\n",
      "14/14 - 0s - 5ms/step - loss: 178.6962 - val_loss: 360.8632\n",
      "Epoch 386/500\n",
      "14/14 - 0s - 5ms/step - loss: 178.4998 - val_loss: 360.6668\n",
      "Epoch 387/500\n",
      "14/14 - 0s - 5ms/step - loss: 178.3034 - val_loss: 360.4705\n",
      "Epoch 388/500\n",
      "14/14 - 0s - 5ms/step - loss: 178.1071 - val_loss: 360.2742\n",
      "Epoch 389/500\n",
      "14/14 - 0s - 5ms/step - loss: 177.9108 - val_loss: 360.0778\n",
      "Epoch 390/500\n",
      "14/14 - 0s - 5ms/step - loss: 177.7145 - val_loss: 359.8815\n",
      "Epoch 391/500\n",
      "14/14 - 0s - 5ms/step - loss: 177.5181 - val_loss: 359.6852\n",
      "Epoch 392/500\n",
      "14/14 - 0s - 5ms/step - loss: 177.3218 - val_loss: 359.4888\n",
      "Epoch 393/500\n",
      "14/14 - 0s - 5ms/step - loss: 177.1255 - val_loss: 359.2925\n",
      "Epoch 394/500\n",
      "14/14 - 0s - 5ms/step - loss: 176.9292 - val_loss: 359.0962\n",
      "Epoch 395/500\n",
      "14/14 - 0s - 5ms/step - loss: 176.7328 - val_loss: 358.8999\n",
      "Epoch 396/500\n",
      "14/14 - 0s - 5ms/step - loss: 176.5365 - val_loss: 358.7036\n",
      "Epoch 397/500\n",
      "14/14 - 0s - 5ms/step - loss: 176.3402 - val_loss: 358.5073\n",
      "Epoch 398/500\n",
      "14/14 - 0s - 5ms/step - loss: 176.1439 - val_loss: 358.3110\n",
      "Epoch 399/500\n",
      "14/14 - 0s - 5ms/step - loss: 175.9476 - val_loss: 358.1147\n",
      "Epoch 400/500\n",
      "14/14 - 0s - 5ms/step - loss: 175.7513 - val_loss: 357.9184\n",
      "Epoch 401/500\n",
      "14/14 - 0s - 5ms/step - loss: 175.5550 - val_loss: 357.7221\n",
      "Epoch 402/500\n",
      "14/14 - 0s - 5ms/step - loss: 175.3588 - val_loss: 357.5258\n",
      "Epoch 403/500\n",
      "14/14 - 0s - 5ms/step - loss: 175.1625 - val_loss: 357.3296\n",
      "Epoch 404/500\n",
      "14/14 - 0s - 5ms/step - loss: 174.9662 - val_loss: 357.1333\n",
      "Epoch 405/500\n",
      "14/14 - 0s - 5ms/step - loss: 174.7699 - val_loss: 356.9370\n",
      "Epoch 406/500\n",
      "14/14 - 0s - 5ms/step - loss: 174.5737 - val_loss: 356.7408\n",
      "Epoch 407/500\n",
      "14/14 - 0s - 5ms/step - loss: 174.3774 - val_loss: 356.5445\n",
      "Epoch 408/500\n",
      "14/14 - 0s - 5ms/step - loss: 174.1812 - val_loss: 356.3482\n",
      "Epoch 409/500\n",
      "14/14 - 0s - 5ms/step - loss: 173.9849 - val_loss: 356.1520\n",
      "Epoch 410/500\n",
      "14/14 - 0s - 5ms/step - loss: 173.7887 - val_loss: 355.9557\n",
      "Epoch 411/500\n",
      "14/14 - 0s - 5ms/step - loss: 173.5924 - val_loss: 355.7595\n",
      "Epoch 412/500\n",
      "14/14 - 0s - 5ms/step - loss: 173.3961 - val_loss: 355.5633\n",
      "Epoch 413/500\n",
      "14/14 - 0s - 5ms/step - loss: 173.1999 - val_loss: 355.3670\n",
      "Epoch 414/500\n",
      "14/14 - 0s - 5ms/step - loss: 173.0037 - val_loss: 355.1707\n",
      "Epoch 415/500\n",
      "14/14 - 0s - 5ms/step - loss: 172.8074 - val_loss: 354.9745\n",
      "Epoch 416/500\n",
      "14/14 - 0s - 5ms/step - loss: 172.6112 - val_loss: 354.7783\n",
      "Epoch 417/500\n",
      "14/14 - 0s - 5ms/step - loss: 172.4150 - val_loss: 354.5821\n",
      "Epoch 418/500\n",
      "14/14 - 0s - 5ms/step - loss: 172.2188 - val_loss: 354.3859\n",
      "Epoch 419/500\n",
      "14/14 - 0s - 5ms/step - loss: 172.0226 - val_loss: 354.1897\n",
      "Epoch 420/500\n",
      "14/14 - 0s - 5ms/step - loss: 171.8263 - val_loss: 353.9935\n",
      "Epoch 421/500\n",
      "14/14 - 0s - 5ms/step - loss: 171.6301 - val_loss: 353.7972\n",
      "Epoch 422/500\n",
      "14/14 - 0s - 5ms/step - loss: 171.4339 - val_loss: 353.6010\n",
      "Epoch 423/500\n",
      "14/14 - 0s - 5ms/step - loss: 171.2377 - val_loss: 353.4048\n",
      "Epoch 424/500\n",
      "14/14 - 0s - 5ms/step - loss: 171.0415 - val_loss: 353.2086\n",
      "Epoch 425/500\n",
      "14/14 - 0s - 5ms/step - loss: 170.8453 - val_loss: 353.0124\n",
      "Epoch 426/500\n",
      "14/14 - 0s - 5ms/step - loss: 170.6491 - val_loss: 352.8162\n",
      "Epoch 427/500\n",
      "14/14 - 0s - 5ms/step - loss: 170.4529 - val_loss: 352.6200\n",
      "Epoch 428/500\n",
      "14/14 - 0s - 5ms/step - loss: 170.2567 - val_loss: 352.4238\n",
      "Epoch 429/500\n",
      "14/14 - 0s - 5ms/step - loss: 170.0605 - val_loss: 352.2276\n",
      "Epoch 430/500\n",
      "14/14 - 0s - 5ms/step - loss: 169.8643 - val_loss: 352.0314\n",
      "Epoch 431/500\n",
      "14/14 - 0s - 5ms/step - loss: 169.6681 - val_loss: 351.8352\n",
      "Epoch 432/500\n",
      "14/14 - 0s - 5ms/step - loss: 169.4719 - val_loss: 351.6390\n",
      "Epoch 433/500\n",
      "14/14 - 0s - 5ms/step - loss: 169.2757 - val_loss: 351.4428\n",
      "Epoch 434/500\n",
      "14/14 - 0s - 5ms/step - loss: 169.0795 - val_loss: 351.2467\n",
      "Epoch 435/500\n",
      "14/14 - 0s - 5ms/step - loss: 168.8834 - val_loss: 351.0505\n",
      "Epoch 436/500\n",
      "14/14 - 0s - 5ms/step - loss: 168.6872 - val_loss: 350.8543\n",
      "Epoch 437/500\n",
      "14/14 - 0s - 5ms/step - loss: 168.4910 - val_loss: 350.6582\n",
      "Epoch 438/500\n",
      "14/14 - 0s - 5ms/step - loss: 168.2948 - val_loss: 350.4620\n",
      "Epoch 439/500\n",
      "14/14 - 0s - 5ms/step - loss: 168.0987 - val_loss: 350.2658\n",
      "Epoch 440/500\n",
      "14/14 - 0s - 5ms/step - loss: 167.9025 - val_loss: 350.0697\n",
      "Epoch 441/500\n",
      "14/14 - 0s - 5ms/step - loss: 167.7064 - val_loss: 349.8735\n",
      "Epoch 442/500\n",
      "14/14 - 0s - 5ms/step - loss: 167.5102 - val_loss: 349.6773\n",
      "Epoch 443/500\n",
      "14/14 - 0s - 5ms/step - loss: 167.3141 - val_loss: 349.4812\n",
      "Epoch 444/500\n",
      "14/14 - 0s - 5ms/step - loss: 167.1179 - val_loss: 349.2850\n",
      "Epoch 445/500\n",
      "14/14 - 0s - 5ms/step - loss: 166.9218 - val_loss: 349.0889\n",
      "Epoch 446/500\n",
      "14/14 - 0s - 6ms/step - loss: 166.7256 - val_loss: 348.8928\n",
      "Epoch 447/500\n",
      "14/14 - 0s - 5ms/step - loss: 166.5295 - val_loss: 348.6967\n",
      "Epoch 448/500\n",
      "14/14 - 0s - 5ms/step - loss: 166.3334 - val_loss: 348.5005\n",
      "Epoch 449/500\n",
      "14/14 - 0s - 5ms/step - loss: 166.1372 - val_loss: 348.3043\n",
      "Epoch 450/500\n",
      "14/14 - 0s - 5ms/step - loss: 165.9411 - val_loss: 348.1082\n",
      "Epoch 451/500\n",
      "14/14 - 0s - 5ms/step - loss: 165.7449 - val_loss: 347.9121\n",
      "Epoch 452/500\n",
      "14/14 - 0s - 5ms/step - loss: 165.5488 - val_loss: 347.7160\n",
      "Epoch 453/500\n",
      "14/14 - 0s - 5ms/step - loss: 165.3527 - val_loss: 347.5199\n",
      "Epoch 454/500\n",
      "14/14 - 0s - 5ms/step - loss: 165.1566 - val_loss: 347.3238\n",
      "Epoch 455/500\n",
      "14/14 - 0s - 5ms/step - loss: 164.9605 - val_loss: 347.1276\n",
      "Epoch 456/500\n",
      "14/14 - 0s - 5ms/step - loss: 164.7643 - val_loss: 346.9315\n",
      "Epoch 457/500\n",
      "14/14 - 0s - 5ms/step - loss: 164.5682 - val_loss: 346.7354\n",
      "Epoch 458/500\n",
      "14/14 - 0s - 5ms/step - loss: 164.3721 - val_loss: 346.5393\n",
      "Epoch 459/500\n",
      "14/14 - 0s - 5ms/step - loss: 164.1760 - val_loss: 346.3432\n",
      "Epoch 460/500\n",
      "14/14 - 0s - 5ms/step - loss: 163.9799 - val_loss: 346.1470\n",
      "Epoch 461/500\n",
      "14/14 - 0s - 5ms/step - loss: 163.7837 - val_loss: 345.9509\n",
      "Epoch 462/500\n",
      "14/14 - 0s - 5ms/step - loss: 163.5876 - val_loss: 345.7548\n",
      "Epoch 463/500\n",
      "14/14 - 0s - 5ms/step - loss: 163.3915 - val_loss: 345.5587\n",
      "Epoch 464/500\n",
      "14/14 - 0s - 5ms/step - loss: 163.1954 - val_loss: 345.3626\n",
      "Epoch 465/500\n",
      "14/14 - 0s - 5ms/step - loss: 162.9993 - val_loss: 345.1665\n",
      "Epoch 466/500\n",
      "14/14 - 0s - 5ms/step - loss: 162.8032 - val_loss: 344.9703\n",
      "Epoch 467/500\n",
      "14/14 - 0s - 5ms/step - loss: 162.6071 - val_loss: 344.7743\n",
      "Epoch 468/500\n",
      "14/14 - 0s - 5ms/step - loss: 162.4110 - val_loss: 344.5782\n",
      "Epoch 469/500\n",
      "14/14 - 0s - 5ms/step - loss: 162.2149 - val_loss: 344.3820\n",
      "Epoch 470/500\n",
      "14/14 - 0s - 5ms/step - loss: 162.0188 - val_loss: 344.1859\n",
      "Epoch 471/500\n",
      "14/14 - 0s - 5ms/step - loss: 161.8227 - val_loss: 343.9898\n",
      "Epoch 472/500\n",
      "14/14 - 0s - 5ms/step - loss: 161.6266 - val_loss: 343.7938\n",
      "Epoch 473/500\n",
      "14/14 - 0s - 5ms/step - loss: 161.4305 - val_loss: 343.5977\n",
      "Epoch 474/500\n",
      "14/14 - 0s - 5ms/step - loss: 161.2344 - val_loss: 343.4016\n",
      "Epoch 475/500\n",
      "14/14 - 0s - 5ms/step - loss: 161.0383 - val_loss: 343.2055\n",
      "Epoch 476/500\n",
      "14/14 - 0s - 5ms/step - loss: 160.8422 - val_loss: 343.0094\n",
      "Epoch 477/500\n",
      "14/14 - 0s - 5ms/step - loss: 160.6461 - val_loss: 342.8133\n",
      "Epoch 478/500\n",
      "14/14 - 0s - 5ms/step - loss: 160.4501 - val_loss: 342.6172\n",
      "Epoch 479/500\n",
      "14/14 - 0s - 5ms/step - loss: 160.2540 - val_loss: 342.4211\n",
      "Epoch 480/500\n",
      "14/14 - 0s - 6ms/step - loss: 160.0579 - val_loss: 342.2251\n",
      "Epoch 481/500\n",
      "14/14 - 0s - 5ms/step - loss: 159.8618 - val_loss: 342.0291\n",
      "Epoch 482/500\n",
      "14/14 - 0s - 5ms/step - loss: 159.6657 - val_loss: 341.8329\n",
      "Epoch 483/500\n",
      "14/14 - 0s - 5ms/step - loss: 159.4696 - val_loss: 341.6368\n",
      "Epoch 484/500\n",
      "14/14 - 0s - 5ms/step - loss: 159.2736 - val_loss: 341.4408\n",
      "Epoch 485/500\n",
      "14/14 - 0s - 5ms/step - loss: 159.0775 - val_loss: 341.2447\n",
      "Epoch 486/500\n",
      "14/14 - 0s - 5ms/step - loss: 158.8814 - val_loss: 341.0486\n",
      "Epoch 487/500\n",
      "14/14 - 0s - 5ms/step - loss: 158.6853 - val_loss: 340.8525\n",
      "Epoch 488/500\n",
      "14/14 - 0s - 5ms/step - loss: 158.4893 - val_loss: 340.6565\n",
      "Epoch 489/500\n",
      "14/14 - 0s - 5ms/step - loss: 158.2932 - val_loss: 340.4604\n",
      "Epoch 490/500\n",
      "14/14 - 0s - 5ms/step - loss: 158.0971 - val_loss: 340.2643\n",
      "Epoch 491/500\n",
      "14/14 - 0s - 5ms/step - loss: 157.9010 - val_loss: 340.0682\n",
      "Epoch 492/500\n",
      "14/14 - 0s - 5ms/step - loss: 157.7050 - val_loss: 339.8722\n",
      "Epoch 493/500\n",
      "14/14 - 0s - 5ms/step - loss: 157.5089 - val_loss: 339.6761\n",
      "Epoch 494/500\n",
      "14/14 - 0s - 4ms/step - loss: 157.3128 - val_loss: 339.4800\n",
      "Epoch 495/500\n",
      "14/14 - 0s - 5ms/step - loss: 157.1168 - val_loss: 339.2839\n",
      "Epoch 496/500\n",
      "14/14 - 0s - 5ms/step - loss: 156.9207 - val_loss: 339.0879\n",
      "Epoch 497/500\n",
      "14/14 - 0s - 5ms/step - loss: 156.7246 - val_loss: 338.8918\n",
      "Epoch 498/500\n",
      "14/14 - 0s - 5ms/step - loss: 156.5285 - val_loss: 338.6957\n",
      "Epoch 499/500\n",
      "14/14 - 0s - 5ms/step - loss: 156.3325 - val_loss: 338.4997\n",
      "Epoch 500/500\n",
      "14/14 - 0s - 5ms/step - loss: 156.1364 - val_loss: 338.3036\n",
      "Test MSE: 338.3036\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "# 1. Load real dataset: Monthly Airline Passengers\n",
    "csv_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'\n",
    "df = pd.read_csv(csv_url, usecols=[1], engine='python')\n",
    "series = df.values.astype('float32')\n",
    "\n",
    "# 2. Prepare sliding windows\n",
    "def make_windows(data, window_size=12):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size): # 50 - 12 = 38\n",
    "        # take `window_size` months as input\n",
    "        X.append(data[i : i + window_size]) # 0-11, 1-12, ..., \n",
    "        # predict the next month\n",
    "        y.append(data[i + window_size]) # 12, 13, ..., 50\n",
    "    # reshape to RNN input: (samples, timesteps, features)\n",
    "    return np.array(X)[..., np.newaxis], np.array(y)\n",
    "\n",
    "window_size = 12\n",
    "X, y = make_windows(series, window_size)\n",
    "# train/test split (80/20)\n",
    "split = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# 3. Build RNN model\n",
    "model = models.Sequential([\n",
    "    # LSTM retains a hidden state across timesteps → captures long-term patterns\n",
    "    layers.LSTM( # LSTM  is better it fixes exploding/vanishing gradients\n",
    "        32,\n",
    "        input_shape=(window_size, 1),\n",
    "        return_sequences=False,  # False: output only final state (many-to-one)\n",
    "    ),\n",
    "    # Dense layer to map final hidden state → forecast value\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 4. Why this is different:\n",
    "#    • Input is 3D; CNN/FFNN would flatten time → lose order info.\n",
    "#    • LSTM’s gates carry context across time; CNN uses local kernels, FFNN has no memory.\n",
    "#    • return_sequences=True would output at every step (seq2seq); False gives just final forecast.\n",
    "\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()  # note shape transition: (batch, window, features) → (batch, units)\n",
    "\n",
    "# 5. Train \n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=500,            \n",
    "    batch_size=8,      \n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 6. Evaluate\n",
    "mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test MSE: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6875add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guiku\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,705</span> (49.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,705\u001b[0m (49.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,705</span> (49.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,705\u001b[0m (49.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 - 1s - 111ms/step - loss: 276.8538\n",
      "Epoch 2/50\n",
      "8/8 - 0s - 5ms/step - loss: 275.7353\n",
      "Epoch 3/50\n",
      "8/8 - 0s - 5ms/step - loss: 275.1496\n",
      "Epoch 4/50\n",
      "8/8 - 0s - 6ms/step - loss: 274.8647\n",
      "Epoch 5/50\n",
      "8/8 - 0s - 5ms/step - loss: 274.4767\n",
      "Epoch 6/50\n",
      "8/8 - 0s - 5ms/step - loss: 274.1859\n",
      "Epoch 7/50\n",
      "8/8 - 0s - 5ms/step - loss: 273.8041\n",
      "Epoch 8/50\n",
      "8/8 - 0s - 5ms/step - loss: 273.4652\n",
      "Epoch 9/50\n",
      "8/8 - 0s - 5ms/step - loss: 273.2942\n",
      "Epoch 10/50\n",
      "8/8 - 0s - 6ms/step - loss: 272.8000\n",
      "Epoch 11/50\n",
      "8/8 - 0s - 5ms/step - loss: 272.5898\n",
      "Epoch 12/50\n",
      "8/8 - 0s - 5ms/step - loss: 272.3807\n",
      "Epoch 13/50\n",
      "8/8 - 0s - 5ms/step - loss: 272.2348\n",
      "Epoch 14/50\n",
      "8/8 - 0s - 5ms/step - loss: 272.2924\n",
      "Epoch 15/50\n",
      "8/8 - 0s - 5ms/step - loss: 272.1329\n",
      "Epoch 16/50\n",
      "8/8 - 0s - 5ms/step - loss: 271.8147\n",
      "Epoch 17/50\n",
      "8/8 - 0s - 5ms/step - loss: 271.6835\n",
      "Epoch 18/50\n",
      "8/8 - 0s - 5ms/step - loss: 271.2656\n",
      "Epoch 19/50\n",
      "8/8 - 0s - 5ms/step - loss: 271.1590\n",
      "Epoch 20/50\n",
      "8/8 - 0s - 5ms/step - loss: 270.8826\n",
      "Epoch 21/50\n",
      "8/8 - 0s - 5ms/step - loss: 270.6273\n",
      "Epoch 22/50\n",
      "8/8 - 0s - 6ms/step - loss: 270.0817\n",
      "Epoch 23/50\n",
      "8/8 - 0s - 5ms/step - loss: 269.8037\n",
      "Epoch 24/50\n",
      "8/8 - 0s - 5ms/step - loss: 269.8062\n",
      "Epoch 25/50\n",
      "8/8 - 0s - 5ms/step - loss: 269.3388\n",
      "Epoch 26/50\n",
      "8/8 - 0s - 5ms/step - loss: 269.1121\n",
      "Epoch 27/50\n",
      "8/8 - 0s - 5ms/step - loss: 268.8555\n",
      "Epoch 28/50\n",
      "8/8 - 0s - 6ms/step - loss: 268.6483\n",
      "Epoch 29/50\n",
      "8/8 - 0s - 6ms/step - loss: 268.3056\n",
      "Epoch 30/50\n",
      "8/8 - 0s - 5ms/step - loss: 268.1344\n",
      "Epoch 31/50\n",
      "8/8 - 0s - 5ms/step - loss: 267.8324\n",
      "Epoch 32/50\n",
      "8/8 - 0s - 5ms/step - loss: 267.7223\n",
      "Epoch 33/50\n",
      "8/8 - 0s - 5ms/step - loss: 267.5065\n",
      "Epoch 34/50\n",
      "8/8 - 0s - 5ms/step - loss: 267.3328\n",
      "Epoch 35/50\n",
      "8/8 - 0s - 5ms/step - loss: 267.1472\n",
      "Epoch 36/50\n",
      "8/8 - 0s - 5ms/step - loss: 266.8308\n",
      "Epoch 37/50\n",
      "8/8 - 0s - 5ms/step - loss: 266.6869\n",
      "Epoch 38/50\n",
      "8/8 - 0s - 5ms/step - loss: 266.7412\n",
      "Epoch 39/50\n",
      "8/8 - 0s - 5ms/step - loss: 265.9679\n",
      "Epoch 40/50\n",
      "8/8 - 0s - 5ms/step - loss: 265.9748\n",
      "Epoch 41/50\n",
      "8/8 - 0s - 5ms/step - loss: 265.9803\n",
      "Epoch 42/50\n",
      "8/8 - 0s - 5ms/step - loss: 265.6685\n",
      "Epoch 43/50\n",
      "8/8 - 0s - 5ms/step - loss: 265.7958\n",
      "Epoch 44/50\n",
      "8/8 - 0s - 5ms/step - loss: 265.4512\n",
      "Epoch 45/50\n",
      "8/8 - 0s - 5ms/step - loss: 265.3374\n",
      "Epoch 46/50\n",
      "8/8 - 0s - 5ms/step - loss: 265.2534\n",
      "Epoch 47/50\n",
      "8/8 - 0s - 5ms/step - loss: 265.2384\n",
      "Epoch 48/50\n",
      "8/8 - 0s - 5ms/step - loss: 264.7932\n",
      "Epoch 49/50\n",
      "8/8 - 0s - 5ms/step - loss: 264.6492\n",
      "Epoch 50/50\n",
      "8/8 - 0s - 5ms/step - loss: 264.3175\n",
      "Dataset MSE: 264.2833\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# 1. Load real data: Monthly Airline Passengers (1949–1960)\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'\n",
    "series = pd.read_csv(url, usecols=[1]).values.astype('float32')\n",
    "\n",
    "# 2. Create sliding windows & labels via tf.data\n",
    "window_size = 12              # past 12 months → predict next month\n",
    "batch_size  = 16\n",
    "\n",
    "# features=series[:-window_size], targets=series[window_size:]\n",
    "dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    data=series[:-window_size],        # all but last window\n",
    "    targets=series[window_size:],      # shifted by window_size\n",
    "    sequence_length=window_size,       # length of each input sequence\n",
    "    sequence_stride=1,                 # slide by 1\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# 3. Build Conv1D → LSTM model\n",
    "model = models.Sequential([\n",
    "    # Conv1D: extracts local patterns (like a CNN over time),\n",
    "    # preserving temporal order vs. FFNN which would flatten it away.\n",
    "    layers.Conv1D(\n",
    "        filters=64, \n",
    "        kernel_size=3, \n",
    "        activation='relu',\n",
    "        input_shape=(window_size, 1)\n",
    "    ),\n",
    "    layers.MaxPooling1D(pool_size=2),   # downsample time dimension\n",
    "\n",
    "    # LSTM: carries hidden state across timesteps → captures long-term dependencies\n",
    "    layers.LSTM(32, return_sequences=False),\n",
    "\n",
    "    # Dropout layer for regularization (common in RNNs to prevent overfitting)\n",
    "    layers.Dropout(0.2),\n",
    "\n",
    "    # Dense head: maps final hidden state → single forecast value\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()  # see how 3D inputs flow through the Conv1D & LSTM\n",
    "\n",
    "# 4. Train briefly (students will later replace with Optuna-driven tuning)\n",
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs=50,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 5. Quick evaluation on a held-out split\n",
    "#    (for simplicity, reuse part of the dataset; in practice, create a separate test set)\n",
    "mse = model.evaluate(dataset, verbose=0)\n",
    "print(f\"Dataset MSE: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7dae491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guiku\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,705</span> (49.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,705\u001b[0m (49.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,705</span> (49.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,705\u001b[0m (49.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 - 1s - 204ms/step - loss: 239.4190 - val_loss: 414.7872\n",
      "Epoch 2/50\n",
      "6/6 - 0s - 12ms/step - loss: 238.7630 - val_loss: 413.4792\n",
      "Epoch 3/50\n",
      "6/6 - 0s - 12ms/step - loss: 237.5968 - val_loss: 413.1910\n",
      "Epoch 4/50\n",
      "6/6 - 0s - 12ms/step - loss: 237.4073 - val_loss: 413.0770\n",
      "Epoch 5/50\n",
      "6/6 - 0s - 11ms/step - loss: 237.2396 - val_loss: 412.8484\n",
      "Epoch 6/50\n",
      "6/6 - 0s - 11ms/step - loss: 237.1244 - val_loss: 412.7717\n",
      "Epoch 7/50\n",
      "6/6 - 0s - 12ms/step - loss: 237.0809 - val_loss: 412.6941\n",
      "Epoch 8/50\n",
      "6/6 - 0s - 11ms/step - loss: 236.9729 - val_loss: 412.6161\n",
      "Epoch 9/50\n",
      "6/6 - 0s - 12ms/step - loss: 236.7840 - val_loss: 412.5379\n",
      "Epoch 10/50\n",
      "6/6 - 0s - 12ms/step - loss: 236.7607 - val_loss: 412.4349\n",
      "Epoch 11/50\n",
      "6/6 - 0s - 12ms/step - loss: 236.5496 - val_loss: 412.2802\n",
      "Epoch 12/50\n",
      "6/6 - 0s - 11ms/step - loss: 236.5606 - val_loss: 412.2009\n",
      "Epoch 13/50\n",
      "6/6 - 0s - 12ms/step - loss: 236.4436 - val_loss: 412.1217\n",
      "Epoch 14/50\n",
      "6/6 - 0s - 11ms/step - loss: 236.3391 - val_loss: 412.0359\n",
      "Epoch 15/50\n",
      "6/6 - 0s - 11ms/step - loss: 236.2889 - val_loss: 411.8825\n",
      "Epoch 16/50\n",
      "6/6 - 0s - 12ms/step - loss: 236.1712 - val_loss: 411.7936\n",
      "Epoch 17/50\n",
      "6/6 - 0s - 12ms/step - loss: 236.1242 - val_loss: 411.7048\n",
      "Epoch 18/50\n",
      "6/6 - 0s - 12ms/step - loss: 235.9086 - val_loss: 411.6159\n",
      "Epoch 19/50\n",
      "6/6 - 0s - 11ms/step - loss: 235.9139 - val_loss: 411.5274\n",
      "Epoch 20/50\n",
      "6/6 - 0s - 11ms/step - loss: 235.5673 - val_loss: 411.0465\n",
      "Epoch 21/50\n",
      "6/6 - 0s - 11ms/step - loss: 235.3026 - val_loss: 410.9463\n",
      "Epoch 22/50\n",
      "6/6 - 0s - 11ms/step - loss: 235.2783 - val_loss: 410.8461\n",
      "Epoch 23/50\n",
      "6/6 - 0s - 12ms/step - loss: 235.1358 - val_loss: 410.7467\n",
      "Epoch 24/50\n",
      "6/6 - 0s - 12ms/step - loss: 235.0029 - val_loss: 410.6482\n",
      "Epoch 25/50\n",
      "6/6 - 0s - 11ms/step - loss: 234.8883 - val_loss: 410.5505\n",
      "Epoch 26/50\n",
      "6/6 - 0s - 11ms/step - loss: 234.8175 - val_loss: 410.4538\n",
      "Epoch 27/50\n",
      "6/6 - 0s - 11ms/step - loss: 234.6831 - val_loss: 410.3573\n",
      "Epoch 28/50\n",
      "6/6 - 0s - 11ms/step - loss: 234.6049 - val_loss: 410.2135\n",
      "Epoch 29/50\n",
      "6/6 - 0s - 12ms/step - loss: 234.4409 - val_loss: 410.1156\n",
      "Epoch 30/50\n",
      "6/6 - 0s - 11ms/step - loss: 234.3616 - val_loss: 410.0176\n",
      "Epoch 31/50\n",
      "6/6 - 0s - 12ms/step - loss: 234.3641 - val_loss: 409.9199\n",
      "Epoch 32/50\n",
      "6/6 - 0s - 11ms/step - loss: 234.3040 - val_loss: 409.8236\n",
      "Epoch 33/50\n",
      "6/6 - 0s - 12ms/step - loss: 234.1026 - val_loss: 409.7281\n",
      "Epoch 34/50\n",
      "6/6 - 0s - 11ms/step - loss: 233.9373 - val_loss: 409.6323\n",
      "Epoch 35/50\n",
      "6/6 - 0s - 11ms/step - loss: 233.8101 - val_loss: 409.5363\n",
      "Epoch 36/50\n",
      "6/6 - 0s - 11ms/step - loss: 233.7921 - val_loss: 409.4407\n",
      "Epoch 37/50\n",
      "6/6 - 0s - 11ms/step - loss: 233.6548 - val_loss: 409.2575\n",
      "Epoch 38/50\n",
      "6/6 - 0s - 12ms/step - loss: 233.6947 - val_loss: 409.1613\n",
      "Epoch 39/50\n",
      "6/6 - 0s - 12ms/step - loss: 233.3961 - val_loss: 409.0651\n",
      "Epoch 40/50\n",
      "6/6 - 0s - 11ms/step - loss: 233.2899 - val_loss: 408.9682\n",
      "Epoch 41/50\n",
      "6/6 - 0s - 11ms/step - loss: 233.2190 - val_loss: 408.8719\n",
      "Epoch 42/50\n",
      "6/6 - 0s - 12ms/step - loss: 232.9231 - val_loss: 408.7747\n",
      "Epoch 43/50\n",
      "6/6 - 0s - 11ms/step - loss: 233.0237 - val_loss: 408.6773\n",
      "Epoch 44/50\n",
      "6/6 - 0s - 12ms/step - loss: 232.9759 - val_loss: 408.4916\n",
      "Epoch 45/50\n",
      "6/6 - 0s - 12ms/step - loss: 232.6409 - val_loss: 408.1298\n",
      "Epoch 46/50\n",
      "6/6 - 0s - 12ms/step - loss: 232.4700 - val_loss: 408.0152\n",
      "Epoch 47/50\n",
      "6/6 - 0s - 11ms/step - loss: 232.2142 - val_loss: 407.9010\n",
      "Epoch 48/50\n",
      "6/6 - 0s - 11ms/step - loss: 232.0760 - val_loss: 407.7870\n",
      "Epoch 49/50\n",
      "6/6 - 0s - 11ms/step - loss: 231.8957 - val_loss: 407.6740\n",
      "Epoch 50/50\n",
      "6/6 - 0s - 13ms/step - loss: 231.9782 - val_loss: 407.5619\n",
      "Test MAE: 407.5619\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# 1. Load real data: Monthly Airline Passengers (1949–1960)\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'\n",
    "series = pd.read_csv(url, usecols=[1]).values.astype('float32')\n",
    "\n",
    "# 2. Define train/test split\n",
    "split_fraction = 0.8\n",
    "split_index = int(len(series) * split_fraction)\n",
    "\n",
    "train_series = series[:split_index]\n",
    "test_series  = series[split_index - window_size:]  \n",
    "# note: subtract window_size so the first test window has enough history\n",
    "\n",
    "# 3. Create sliding windows for train and test via tf.data\n",
    "window_size = 12    # past 12 months → predict next month\n",
    "batch_size  = 16\n",
    "\n",
    "train_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    data=train_series[:-window_size],\n",
    "    targets=train_series[window_size:],\n",
    "    sequence_length=window_size,\n",
    "    sequence_stride=1,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "test_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    data=test_series[:-window_size],\n",
    "    targets=test_series[window_size:],\n",
    "    sequence_length=window_size,\n",
    "    sequence_stride=1,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# 4. Build Conv1D → LSTM model\n",
    "model = models.Sequential([\n",
    "    layers.Conv1D(\n",
    "        filters=64, \n",
    "        kernel_size=3, \n",
    "        activation='relu',\n",
    "        input_shape=(window_size, 1)\n",
    "    ),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.LSTM(32, return_sequences=False),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()\n",
    "\n",
    "# 5. Train on the training set\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    verbose=2,\n",
    "    validation_data=test_dataset  # monitor val loss on the test split\n",
    ")\n",
    "\n",
    "# 6. Evaluate on the held-out test set\n",
    "test_loss = model.evaluate(test_dataset, verbose=0)\n",
    "print(f\"Test MAE: {test_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
